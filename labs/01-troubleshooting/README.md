# Troubleshoot your cluster with Gadgets

## Overview

Inspektor Gadget has a powerful set of [gadgets](https://artifacthub.io/packages/search?kind=22&verified_publisher=true&official=true&cncf=true&sort=relevance&page=1) that can be used to troubleshoot and debug issues in your Kubernetes cluster. This lab will focus on highlighting some of the key gadgets
and different ways to use them to troubleshoot issues in your cluster.

### Task 1: Capturing Network Traffic with tcpdump

#### Problem Statement

Debugging network issues in Kubernetes is challenging due to its distributed architecture. Applications run across multiple pods on different nodes, making it difficult to capture and correlate network traffic. Traditional tcpdump requires SSH access to nodes, identifying which node a pod is running on, and manually correlating traffic between pods.

An application in your cluster is experiencing intermittent connection issues when communicating with an external API. The application logs show occasional errors, but it's unclear where the issue lies.

Let's use the [tcpdump gadget](https://inspektor-gadget.io/docs/latest/gadgets/tcpdump) to capture network traffic in the namespace where our app is running. By analyzing the captured packets—DNS queries, TCP handshake, HTTP requests, TLS negotiations—we can identify where the problem is: DNS resolution failures, TCP connection issues, TLS certificate problems, or application-level errors.

#### Step 1: Create a new namespace and deploy a test pod

Start by creating a new namespace called `network-debug` and deploy a pod:

```bash
kubectl create ns network-debug
kubectl -n network-debug run mypod --image=wbitt/network-multitool -- sleep inf
```

Wait for the pod to be ready:

```bash
kubectl -n network-debug get pods
```

```
NAME    READY   STATUS    RESTARTS   AGE
mypod   1/1     Running   0          5s
```

#### Step 2: Start the tcpdump gadget

In a separate terminal, start capturing network traffic:

```bash
kubectl gadget run tcpdump:v0.46.0 -n network-debug
```

#### Step 3: Generate network traffic

In another terminal, generate some HTTP traffic:

```bash
kubectl -n network-debug exec mypod -- curl -s http://example.com.
```

#### Step 4: Observe the captured traffic

Go back to the terminal where tcpdump is running. You should see the network traffic generated by the `curl` command:

```
K8S.NODE                           K8S.NAMESPACE  K8S.PODNAME  K8S.CONTAINERNAME  PA… IFINDEX  PAYLOAD_L…  PACKET_SI…  COMM    PID    TID  PAYLOAD
aks-nodepool1-22418794-vmss000000  network-debug  mypod        mypod              0   23       103         103         curl  66489  66495  <103 bytes>
aks-nodepool1-22418794-vmss000000  network-debug  mypod        mypod              0   23       103         103         curl  66489  66495  <103 bytes>
aks-nodepool1-22418794-vmss000000  network-debug  mypod        mypod              1   23       196         196         curl  66489  66495  <196 bytes>
aks-nodepool1-22418794-vmss000000  network-debug  mypod        mypod              1   23       196         196         curl  66489  66495  <196 bytes>
aks-nodepool1-22418794-vmss000000  network-debug  mypod        mypod              0   23       89          89          curl  66489  66495  <89 bytes>
```

However, by default it shows the packet metadata (not the full packet contents). If you want to let `tcpdump` analyze the captured traffic directly, you can use the `pcap-ng` output mode and pipe Inspektor Gadget's output directly to `tcpdump` like so:

```bash
kubectl gadget run tcpdump:v0.46.0 -n network-debug -o pcap-ng | tcpdump -nr -
```

Now, trigger the curl command again and you should see detailed packet information:

```
reading from file -, link-type EN10MB (Ethernet), snapshot length 262144
00:00:00.000000 IP 10.244.0.230.48989 > 10.0.0.10.53: 34962+ A? example.com. (29)
00:00:00.000000 IP 10.244.0.230.48989 > 10.0.0.10.53: 35162+ AAAA? example.com. (29)
00:00:00.000000 IP 10.0.0.10.53 > 10.244.0.230.48989: 34962* 6/0/0 A 23.192.228.84, A 23.220.75.245, A 23.220.75.232, A 23.192.228.80, A 23.215.0.138, A 23.215.0.136 (191)
00:00:00.000000 IP 10.0.0.10.53 > 10.244.0.230.48989: 35162* 6/0/0 AAAA 2600:1408:ec00:36::1736:7f31, AAAA 2600:1406:bc00:53::b81e:94ce, AAAA 2600:1406:5e00:6::17ce:bc12, AAAA 2600:1406:5e00:6::17ce:bc1b, AAAA 2600:1408:ec00:36::1736:7f24, AAAA 2600:1406:bc00:53::b81e:94c8 (263)
00:00:00.000000 IP 10.244.0.230.50390 > 23.192.228.84.80: Flags [S], seq 3324538669, win 64240, options [mss 1460,sackOK,TS val 1986886729 ecr 0,nop,wscale 7], length 0
00:00:00.000000 IP 23.192.228.84.80 > 10.244.0.230.50390: Flags [S.], seq 1917210305, ack 3324538670, win 65160, options [mss 1460,sackOK,TS val 3748908804 ecr 1986886729,nop,wscale 7], length 0
00:00:00.000000 IP 10.244.0.230.50390 > 23.192.228.84.80: Flags [.], ack 1, win 502, options [nop,nop,TS val 1986886771 ecr 3748908804], length 0
00:00:00.000000 IP 10.244.0.230.50390 > 23.192.228.84.80: Flags [P.], seq 1:76, ack 1, win 502, options [nop,nop,TS val 1986886771 ecr 3748908804], length 75: HTTP: GET / HTTP/1.1
00:00:00.000000 IP 23.192.228.84.80 > 10.244.0.230.50390: Flags [.], ack 76, win 509, options [nop,nop,TS val 3748908847 ecr 1986886771], length 0
00:00:00.000000 IP 23.192.228.84.80 > 10.244.0.230.50390: Flags [P.], seq 1:776, ack 76, win 509, options [nop,nop,TS val 3748908859 ecr 1986886771], length 775: HTTP: HTTP/1.1 200 OK
00:00:00.000000 IP 10.244.0.230.50390 > 23.192.228.84.80: Flags [.], ack 776, win 501, options [nop,nop,TS val 1986886826 ecr 3748908859], length 0
00:00:00.000000 IP 10.244.0.230.50390 > 23.192.228.84.80: Flags [F.], seq 76, ack 776, win 501, options [nop,nop,TS val 1986886826 ecr 3748908859], length 0
00:00:00.000000 IP 23.192.228.84.80 > 10.244.0.230.50390: Flags [F.], seq 776, ack 77, win 509, options [nop,nop,TS val 3748908901 ecr 1986886826], length 0
00:00:00.000000 IP 10.244.0.230.50390 > 23.192.228.84.80: Flags [.], ack 777, win 501, options [nop,nop,TS val 1986886868 ecr 3748908901], length 0
```

This output shows the complete lifecycle of an HTTP request:

1. **DNS Resolution** (lines 1-4): Pod queries DNS for `example.com` and receives multiple IP addresses
2. **TCP Handshake** (lines 5-7): Three-way handshake `[S]`, `[S.]`, `[.]` establishes connection
3. **HTTP Communication** (lines 8-11): Pod sends GET request, server responds with HTTP 200 OK
4. **Connection Teardown** (lines 12-14): Clean connection close with `[F.]` flags

Stop the tcpdump gadget by pressing `Ctrl+C`.

### Challenge

Now let's troubleshoot a more specific scenario. We want to capture only HTTP traffic (port 80) to understand if our application is successfully connecting to external services.

<details>
<summary>Solution</summary>

You could use the tcpdump's filter capabilities to capture only traffic on port 80:

```bash
kubectl gadget run tcpdump:v0.46.0 -n network-debug -o pcap-ng | tcpdump port 80 -nvr -
```

However, we are using `tcpdump` only to display the packets so with this filter Inspektor Gadget will continue to capture all traffic but `tcpdump` will drop non-matching packets.

A more efficient approach is to use Inspektor Gadget's `--pf` flag as is applied directly in the eBPF program, so only matching packets are captured and sent to user space, reducing overhead:

```bash
kubectl gadget run tcpdump:v0.46.0 -n network-debug --pf "port 80" -o pcap-ng | tcpdump -nvr -
```

> [!IMPORTANT]
> The best part is that the `pf` filter supports standard tcpdump filter expressions, for example:
>
> - Capture only DNS traffic: `--pf "port 53"`
> - Capture traffic to a specific host: `--pf "host 93.184.215.14"`
> - Capture only TCP traffic: `--pf "tcp"`
> - Combine filters: `--pf "tcp and port 443"`

</details>

#### Step 5: Clean up

```bash
kubectl delete ns network-debug
```

## Task 2: Troubleshooting File System Performance Issues

### Problem Statement

Our AI model training jobs are taking much longer than expected. The training pods are supposed to save model checkpoints periodically, but we're seeing delays during the save operations. This is significantly increasing our training costs and slowing down the training process.

### Initial Investigation

We've already checked the basics:

- ✅ CPU and memory usage look normal in dashboards
- ✅ The actual training computation is running fine
- ❓ **Checkpointing operations are the bottleneck**

Let's investigate what's happening during the file save operations.

### Step 1: Deploy the ML training workload

Let's run our training job:

```bash
kubectl apply -f training-job.yaml
```

### Step 2: Verify the pod is running

```bash
kubectl get pods -n training
```

You should see output like this:

```
NAME           READY   STATUS    RESTARTS   AGE
training-job   1/1     Running   0          32s
```

### Step 3: Investigation with Inspektor Gadget

Let's first check the latency of file system operations with [trace_fsslower](https://inspektor-gadget.io/docs/latest/gadgets/trace_fsslower/):

```bash
kubectl gadget run trace_fsslower:v0.46.0 --namespace training --min 0 --fields k8s.podName,proc.comm,op,file,delta_us
```

Where:

- `--namespace training`: Specify the namespace where our training job is running
- `--min 0`: As we don't know yet the exact threshold of slowness, let's keep the threshold at `0` to capture all file operations regardless of latency
- `--fields k8s.podName,proc.comm,op,file,delta_us`: Customize the output fields to show pod name, process name, operation type, file name, and latency in microseconds

The gadget will output file system operations in real-time like this:

```
K8S.PODNAME   COMM  OP       FILE                DELTA_US
training-job  dd    F_OPEN   ld.so.cache                2
training-job  dd    F_OPEN   libc.so.6                  1
training-job  dd    F_READ   libc.so.6                  2
training-job  dd    F_READ   libc.so.6                  0
training-job  dd    F_READ   libc.so.6                  0
training-job  dd    F_OPEN   model_epoch_2.pkl          2
training-job  dd    F_WRITE  model_epoch_2.pkl       1030
training-job  dd    F_WRITE  model_epoch_2.pkl        989
training-job  dd    F_WRITE  model_epoch_2.pkl        906
training-job  dd    F_WRITE  model_epoch_2.pkl        941
training-job  dd    F_WRITE  model_epoch_2.pkl        927
training-job  dd    F_WRITE  model_epoch_2.pkl        941
training-job  dd    F_WRITE  model_epoch_2.pkl       1138
training-job  dd    F_WRITE  model_epoch_2.pkl        971
training-job  dd    F_WRITE  model_epoch_2.pkl        969
training-job  dd    F_WRITE  model_epoch_2.pkl        986
training-job  dd    F_FSYNC  model_epoch_2.pkl       5789
training-job  bash  F_OPEN   sleep                      3
training-job  bash  F_READ   sleep                      3
```

### Challenge

Several processes are running in the pod, but we are only interested in the `dd` command that is writing the checkpoint files (`model_epoch_*.pkl`).

The first challenge is to use the `--filter` flag to show only operations performed by the `dd` command.

> [!TIP]
> You can use `kubectl gadget run trace_fsslower:v0.46.0 --help` to see how to use the `--filter` option and what `fields` are available to filter on.

<details>
<summary>Solution</summary>

```bash
kubectl gadget run trace_fsslower:v0.46.0 -n training --min 0 --fields k8s.podName,proc.comm,op,file,delta_us --filter 'proc.comm==dd'
```

</details>

The second challenge is to filter only the operations on checkpoint files:

<details>
<summary>Solution</summary>

You can combine multiple conditions in the `--filter` flag:

```bash
kubectl gadget run trace_fsslower:v0.46.0 -n training --min 0 --fields k8s.podName,proc.comm,op,file,delta_us --filter 'proc.comm==dd,file~^model_epoch_'
```

`^model_epoch_` is just an example of a regex to match checkpoint files. Additionally, only filtering by `file~^model_epoch_` is also sufficient since only `dd` is writing those files.
</details>

**Analysis**: We can see the following values:

- Most `F_WRITE` operations: Around 1ms
- The `F_FSYNC` operation: Around 6ms

We know that `F_FSYNC` operations are typically slower than writes. However, these numbers alone don't tell us whether the disk hardware is the bottleneck or if something else (like CPU bottlenecks) is causing the delays.

To understand the root cause, we need to measure the disk's actual hardware performance.

### Step 4: Check Disk Performance

Let's use `profile_blockio` to understand the underlying disk performance:

```bash
kubectl gadget run profile_blockio:v0.46.0 -n training --timeout 30
```

The `profile_blockio` gadget measures **actual disk I/O latency at the hardware level**. It captures the time from when a request is sent to the storage device until the device completes the operation.

> [!IMPORTANT]
> This measurement excludes time spent in kernel queues, CPU processing, or application-level delays. It only shows the pure storage device performance - essentially answering "How fast is our disk hardware?"
>
> Comparison with trace_fsslower:
>
> - **`profile_blockio`**: Measures storage device performance only
> - **`trace_fsslower`**: Measures total application-level file operation time (includes CPU, queuing, and device time)

```text
latency
        µs               : count    distribution
         0 -> 1          : 0        |                                        |
         1 -> 2          : 0        |                                        |
         2 -> 4          : 0        |                                        |
         4 -> 8          : 0        |                                        |
         8 -> 16         : 30       |*                                       |
        16 -> 32         : 75       |****                                    |
        32 -> 64         : 389      |*************************               |
        64 -> 128        : 596      |*************************************** |
       128 -> 256        : 602      |****************************************|
       256 -> 512        : 256      |*****************                       |
       512 -> 1024       : 28       |*                                       |
      1024 -> 2048       : 0        |                                        |
      2048 -> 4096       : 0        |                                        |
      4096 -> 8192       : 6        |                                        |
      8192 -> 16384      : 0        |                                        |
     16384 -> 32768      : 0        |                                        |
     32768 -> 65536      : 0        |                                        |
     65536 -> 131072     : 0        |                                        |
    131072 -> 262144     : 0        |                                        |
    262144 -> 524288     : 0        |                                        |
    524288 -> 1048576    : 0        |                                        |
   1048576 -> 2097152    : 0        |                                        |
   2097152 -> 4194304    : 0        |                                        |
   4194304 -> 8388608    : 0        |                                        |
   8388608 -> 16777216   : 0        |                                        |
  16777216 -> 33554432   : 0        |                                        |
  33554432 -> 67108864   : 0        |                                        |
```

### Step 5: Analysis and Conclusion

Let's compare what each gadget tells us:

**What we observed:**

- **Application operations** (`trace_fsslower`): Most writes around 1ms but fsyncs taking 6ms.
- **Disk hardware** (`profile_blockio`): 0.1-0.5ms consistently

**The gap:** Application operations can be **5-10x slower** than the disk can handle.

**Conclusion:** The disk is not the problem. Something else is slowing down the application before it even gets to the disk.

**Root cause:** For the purpose of this lab, we limited CPU resources for the training job, causing CPU throttling during intensive file operations.

### Step 6: Clean up

```bash
kubectl delete ns training
```

Congratulations! You have learned how to use multiple Inspektor Gadget tools to distinguish between disk performance issues and CPU throttling problems. This methodology is essential for accurate root cause analysis in production environments.

### Next Steps

How would it look if the one being throttled was disk instead of CPU? Explore more with Inspektor Gadget!

We don't know how fast our disk is so we need to profile it over time. To do that, we need to generate some metrics and see how those values change over time. Check the [Monitoring with Prometheus and Grafana lab](../02-monitoring/README.md) to learn how to do that.
